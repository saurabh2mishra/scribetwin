{
  "Saurabh Mishra": "Orchestration with Apache Airflow -A beginner‚Äôs guide for project-based learning !! Photo by Jeffrey Brandjes on Unsplash This blog is a collection of my notes on Airflow. I thought to write it to consolidate all of my learning in one place with a good hands-on project. I have tried to put all concepts/jargon lucidly to make our understanding of the Airflow clear and to the point. However, before you jump and dig into it, there are two prerequisites. üî¥ Python üî¥ Extract-Transform-Load aka ETL And the reason for those prerequisites is apparent ‚Äî Either we build a report or machine learning project, ETL is a must for both, and since Airflow is written in Python, we cannot avoid it. Introduction Airflow is a batch-oriented framework for creating data pipelines. It uses DAG to create data processing networks or pipelines. DAG stands for ‚Äî > Direct Acyclic Graph. It flows in one direction. You can‚Äôt come back to the same point, i.e. acyclic. In many data processing environments, a series of computations are run on the data to prepare it for one or more ultimate destinations. This type of data processing flow is often referred to as a data pipeline. A DAG or data processing flow can have multiple paths, also called branching. The simplest DAG could be like this. where read-data-from-some-endpoint & write-to-storage ‚Äî represent a task (unit of work) Arrow ‚Üí represents processing direction and dependencies to check on what basis the next action will be triggered. Ok, so why should we use Airflow? If you like Everything As Code and everything means everything, including your configurations, then Airflow is the right choice. Everything as a Code mechanism helps to create any complex level pipeline to solve the problem. If you like open source because almost everything you can get as an inbuilt operator or executor. Backfilling features. It enables you to reprocess historical data. And, why shouldn‚Äôt you use Airflow? If you want to build a streaming data pipeline. Airflow Architecture So, we have at least an idea that Airflow is created to build the data pipelines. Below we can see the different components of Airflow and their internal connections. https://airflow.apache.org/docs/apache-airflow/stable/_images/arch-diag-basic.png We can see the above components when we install Airflow, and implicitly Airflow installs them to facilitate the execution of pipelines. These components are DAG directory , to keep all dag in place to be read by scheduler and executor. Scheduler parses DAGS, checks their scheduled interval, and starts scheduling DAGs tasks for execution by passing them to airflow workers. Workers are responsible for doing actual work. It picks up tasks and executes them. Web server presents a handy user interface to inspect, trigger, and debug the behavior of DAGs and tasks. Metadata Database , used by the scheduler, executor, and webserver to store state so that all of them can communicate and take decisions. - follow this link to see, how to set and get metadata variables. See the section on Accessing Metadata Database For now, this is enough architecture. Let‚Äôs move to the next part. Installing Airflow Airflow provides many options for installations. You can read all the options in the official airflow documentation , and then decide which option suits your need. However, to keep it simple and experimental, I will go ahead with the Docker way of installation. Installing Airflow with Docker is simple and intuitive which helps us to understand the typical features and working of Airflow. Below are the pre-requisites for running Airflow in Docker. Docker Community Edition is installed on your machine. Check this link for Windows and Mac . I followed this blog for docker installation on Mac Docker Compose installation. Caveats ‚Äî You need at least 4GB of memory for the Docker engine. Refer to this link if you are trying to install Airflow in windows (I haven‚Äôt tried it myself)‚Äî link1 and link2 Installation Steps (on Mac) Create a file name as airflow_runner.sh. Copy the below commands in the script. docker run --rm \"debian:buster-slim\" bash -c 'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))' curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.4/docker-compose.yaml' mkdir -p ./dags ./logs ./plugins echo -e \"AIRFLOW_UID=$(id -u)\" > .env Provide execute access to file. chmod +x airflow_runner.sh Run source airflow_runner.sh Once the above steps are completed successfully, run docker-compose up airflow-init to initialize the database. After initialization is complete, you should see a message like the one below. airflow-init_1 | Upgrades done airflow-init_1 | Admin user airflow created airflow-init_1 | 2.3.0 start_airflow-init_1 exited with code 0 Now, we are ready to go for the next step. Starting Docker Airflow project üëâ docker-compose up ‚Äî build The above command starts a docker environment and runs below services as well Webserver Scheduler Postgres database for metastore After a few seconds, when everything is up, the webserver is available at http://localhost:8080 . The default account has the login airflow and the password airflow . From the terminal, you can also run docker ps to check the processes which are up and running. Cleaning up To stop and delete containers, delete volumes with database data and download images, run: üëâ docker-compose down ‚Äî volumes ‚Äî rmi all Fundamentals of Airflow We have installed Airflow and know at a high level what it stands for but we are yet to find how to build our first pipeline. Before we start creating the pipeline, we need to touch few more concepts to create a full-fledged project. Let‚Äôs refresh our memory one more time. Airflow works on the DAG principle, and DAG is an acyclic graph. We saw this simple example read-data-from-some-endpoint ‚Üí write-to-storage Let‚Äôs go to step by step to create DAG for the Airflow. Step 1 Create a DAG. It accepts a unique name, when to start it, and what could be the interval of running. There are many more parameters that it agrees to, but let‚Äôs stick with these three. dag = DAG( dag_id=\"my_first_dag\", start_date=airflow.utils.dates.days_ago(2), schedule_interval=None, ) Step 2 And now, we need to create our two functions (I‚Äôm creating dummy functions) and will attach them to the Operator. def read_data_from_some_endpoint(): pass def write_to_storage(): pass Step 3 Let‚Äôs create our operators. We have python functions that need to be attached to some Operators. The last argument is that it accepts a DAG. Here we need to tell the operator which dag it will consider. download_data = PythonOperator(# This is our Airflow Operator. task_id=\"download_data\", #unique name; it could be any name python_callable=read_data_from_some_endpoint, #python function/callable dag = dag #Here we will attach our operator with the dag which we created at 1st step. ) persist_to_storage = PythonOperator( task_id = \"persist_to_storage\", python_callable = write_to_storage, dag = dag ) Step 4 Now, Let's create the execution order of our operators download_data >> persist_to_storage # >> is bit shift operator in python which is overwritten in Airflow to indicate the direction upstream and downstream of task flow. That‚Äôs it. We have successfully created our first DAG. Define your Task and DAG Airflow provides three ways to define your DAG Classical with context manager Decorators It doesn‚Äôt matter which way you define your workflow but sticking to only one helps to debug and review the codebase. Mixing different definitions can perplex the code (though it is a personal choice) import pendulum from airflow import DAG from airflow.operators.dummy import DummyOperator # Classical dag = DAG(\"classical_dag\", start_date=pendulum.datetime(2022, 5, 15, tz=\"UTC\"), schedule_interval=\"@daily\", catchup=False) op = DummyOperator(task_id=\"a-dummy-task\", dag=dag) # with context manager with DAG( \"context_manager_dag\", start_date=pendulum.datetime(2022, 5, 15, tz=\"UTC\"), schedule_interval=\"@daily\", catchup=False ) as dag: op = DummyOperator(task_id=\"a-dummy-task\") # Decorators @dag(start_date=pendulum.datetime(2022, 5, 15, tz=\"UTC\"), schedule_interval=\"@daily\", catchup=False) def generate_decorator_dag(): op = DummyOperator(task_id=\"a-dummy-task\") dag = generate_decorator_dag() How to create a bit more complex task flow? Let‚Äôs take this example. We see two color codes have been used in the above image. light red ‚Äî shows branch flow (two or more flows) i.e. branch_1, branch_2 light green ‚Äî normal task for a different purpose. i.e. false_1, false_2, true_2 etc. Now, without worrying about code, let‚Äôs create the task flow to represent the above structure. 1- Lower workflow from branch_1 branch_1 >> true_1 >> join_1 2- Upper workflow from branch_1 upper flow has two sections. The first part goes till branch_2 branch_1 >> false_1 >> branch_2 and then at branch_2, two parallel execution happens and goes till false_3 branch_2 >> false_2 >> join_2 >> false_3 branch_2 >> true_2 >> join_2 >> false_3 Since false_2 and true_2 is happening in parallel, so we can merge them (put them in a list) in this way branch_2 >> [true_2, false_2] >> join_2 >> false_3 and finally, we can merge the above steps like this branch_1 >> false_1 >> branch_2 >> [true_2, false_2] >> join_2 >> false_3 >> join_1 So we have got these two from step 1 and step 2 branch_1 >> true_1 >> join_1 branch_1 >> false_1 >> branch_2 >> [true_2, false_2] >> join_2 >> false_3 >> join_1 and that‚Äôs represent the execution of task or a DAG. How bit shift operator (>> or <<) defines task dependency? The __rshift__ and __lshift__ methods of the BaseOperator class implements the Python bit shift logical operator in the context of setting a task or a DAG downstream of another. See the implementation here . So, bit shift has been used as syntactic sugar for set_upstream (<<) and set_downstream (>>) tasks. For example task1 >> task2 is same as task2 << task1 is same as task1.set_downstream(task2) is same as task1.set_upstream(task2) This operator plays an important role to build relationships among the tasks. Effective Task Design The created task should follow 1- Atomicity This means either all occur, or nothing happens . So each task should do only one activity and, if not the case, split the functionality into individual tasks. 2- Idempotency An Airflow task is said to be idempotent if calling the same task multiple times with the same inputs has no additional effect . This means that rerunning a task without changing the inputs should not change the overall output. for data load: It can be made idempotent by checking for existing results or ensuring that the task overwrites previous results. for database load: upsert can be used to overwrite or update previous work done on the tables. 3- Back Filling the previous task The property helps to process the historical data. The DAG class can be initiated with property catchup if catchup=False -> Airflow starts processing from the current interval. If catchup=True -> This is the default property. Airflow starts processing from the past interval. Runtime Variables All operators load context a pre-loaded variable to supply the most used variables during the DAG run. Python examples can be shown here from urllib import request import airflow from airflow import DAG from airflow.operators.python import PythonOperator dag = DAG( dag_id=\"showconext\", start_date=airflow.utils.dates.days_ago(1), schedule_interval=\"@hourly\", ) def _show_context(**context): \"\"\" the context contains these preloaded items to pass in dag during runtime. Airflow‚Äôs context dictionary can be found in the get_template_context method, in Airflow‚Äôs models.py. { 'dag': task.dag, 'ds': ds, 'ds_nodash': ds_nodash, 'ts': ts, 'ts_nodash': ts_nodash, 'yesterday_ds': yesterday_ds, 'yesterday_ds_nodash': yesterday_ds_nodash, 'tomorrow_ds': tomorrow_ds, 'tomorrow_ds_nodash': tomorrow_ds_nodash, 'END_DATE': ds, 'end_date': ds, 'dag_run': dag_run, 'run_id': run_id, 'execution_date': self.execution_date, 'prev_execution_date': prev_execution_date, 'next_execution_date': next_execution_date, 'latest_date': ds, 'macros': macros, 'params': params, 'tables': tables, 'task': task, 'task_instance': self, 'ti': self, 'task_instance_key_str': ti_key_str, 'conf': configuration, 'test_mode': self.test_mode, } \"\"\" start = context[\"execution_date\"] end = context[\"next_execution_date\"] print(f\"Start: {start}, end: {end}\") show_context = PythonOperator( task_id=\"show_context\", python_callable=_show_context, dag=dag ) The above variables are pre-loaded under context and can be used anywhere in the operator. Dynamic reference happens in the Jinja templating way. e.g. {{ds}}, {{next_ds}}, {{dag_run}} Templating fields and scripts Two attributes in the BaseOperator define what can we put in for templating. template_fields: Holds the list of variables that are templateable template_ext: Contains a list of file extensions that can be read and templated at runtime See the example for those two fields‚Äô declaration class BashOperator(BaseOperator): # defines which fields are templateable template_fields = ('bash_command', 'env') template_ext = ('.sh', '.bash') # defines which file extensions are templateable def __init__( self, *, bash_command, env: None, output_encoding: 'utf-8', **kwargs, ): super().__init__(**kwargs) self.bash_command = bash_command # templateable (can also give path to .sh or .bash script) self.env = env # templateable self.output_encoding = output_encoding # not templateable Example of DAG which uses Airflow context for templating Let‚Äôs take an example to showcase the power of templating from datetime import datetime BashOperator( task_id=\"print_now\", bash_command=\"echo It is currently {{ macros.datetime.now() }}\", ) üëâ Note If you see here, we used macro to call datetime.now(). If we don‚Äôt use the macro, it will raise jinja2.exceptions.UndefinedError: ‚Äòdatetime‚Äô is undefined exception. Check here for the macro list . But now you might be thinking about where we got PythonOperator , DAG etc. We will see Airflow‚Äôs critical modules to understand it. üëâ This topic is covered in detail in this blog . Airflow Module Structure Airflow has a standard module structure. It has all its important packages under airflow. Few of the essential module structures are here airflow - For DAG and other base API. airflow.executors : For all inbuilt executors. airflow.operators : For all inbuilt operators. airflow.models : For DAG, log error, pool, xcom (cross-communication) etc. airflow.sensors : Different sensors (in simple words, it is either time interval or file watcher to meet some criteria for task executions) airflow.hooks : Provides different modules to connect external API services or databases. So, by looking at the above module, we can quickly determine that to get PythonOperator or any other Operator, we need to import them from airflow.operators . Similarly, an executor can be imported from airflow.executors and so on. Apart from that, many different packages providers, including vendors and third-party enhance the capability of Airflow. All providers follow apache-airflow-providers nomenclatures for the package build. Providers can contain operators, hooks, sensors, and transfer operators to communicate with many external systems, but they can also extend Airflow core with new capabilities. This is the list of providers ‚Äî providers list Workloads Operators Operators help run your function or any executable program. Types of Operators Primarily there are three types of Operators. (i) Operators Helps to trigger certain action. Few of them are PythonOperator ‚Äî To wrap a python callables/functions inside it. BashOperator ‚Äî To call your bash script or command. Within BashOperator we can also call any executable program. DummyOperator ‚Äî to show a dummy task DockerOperator ‚Äî To write and execute docker images. EmailOperator ‚Äî To send an email (using SMTP configuration) and many more operators do exits. See the full operators list in the official documentation. (ii) Sensors A particular type of operator whose purpose is to wait for an event to start the execution. For instance, ExternalTaskSensor waits on another task (in a different DAG) to complete execution. S3KeySensor S3 Key sensors are used to wait for a specific file or directory to be available on an S3 bucket. NamedHivePartitionSensor ‚Äî Waits for a set of partitions to appear in Hive. (iii) Transfers Moves data from one location to another. e.g. MySqlToHiveTransfer Moves data from MySql to Hive. S3ToRedshiftTransfer load files from s3 to Redshift. Scheduler The scheduler is the crucial component of Airflow, as most of the magic happens here. It determines when and how your pipelines are executed. At a high level, the scheduler runs through the following steps. Once users have written their workflows as DAGs, the scheduler reads the files containing these DAGs to extract the corresponding tasks, dependencies, and schedule intervals of each DAG. For each DAG, the scheduler then checks whether the scheduled interval for the DAG has passed since the last time, it was read. If so, the tasks in the DAG are scheduled for execution. For each scheduled task, the scheduler then checks whether the task‚Äôs dependencies (= upstream tasks) have been completed. If so, the task is added to the execution queue. The scheduler waits several moments before starting a new loop by jumping back to step 1. üëâ To start a scheduler, just run the airflow scheduler command. In Airflow, while defining the DAG, we provide a few options to let the scheduler know when jobs are required to be triggered. start_date -> when to start the DAG. end_date -> when to stop the DAG schedule_interval -> Time interval for the subsequent run. hourly, daily, minutes etc depends_on_past -> Boolean to decide from when DAG will execute. retry_delay -> time duration for next retry. It accepts datetime object. e.g. for 2 mins we will write timedelta(minutes=2) Airflow scheduler works on the principle of Cron based job execution. Below is the cron presentation. every 5th minute ‚Äî > */5 * * * * every hour at minute 30 e.g. at 10:30, 11:30, and so on. ‚Äî > 0,5,10 * * * * If you haven‚Äôt worked before on Unix based cron job scheduler, then it is tough to know how to write them exactly (it‚Äôs also tricky for experienced developers as well). Check this website to generate cron expression ‚Äî cron-expression-generator Executors It helps to run the task instance (task instances are functions which we have wrapped under operator) Types of Executors There are two types of executors Local Executors Debug Executor - The DebugExecutor is a debug tool and can be used from IDE. It is a single process executor that queues tasks and executes them. Sequential Executor ‚Äî Default executor and runs within scheduler. Apart from this, it executes one task instance at a time, which eventually makes it not a good candidate for production. Local Executor ‚Äî Run within scheduler and execute multiple tasks instance at a time. Again not a good candidate for production as it doesn‚Äôt scale. Remote Executors Celery Executor ‚Äî Run tasks on dedicated machines(workers). It uses distributed task queue to distribute loads to different workers to parallelise work. It horizontally scales, making it fault-tolerant and a good candidate for production. Kubernetes Executor ‚Äî Run tasks in dedicated POD(worker), and Kubernetes APIs get used to managing the POD. It scales efficiently and is a perfect candidate for production. LocalKubernetes Executor ‚Äî Local kubernetes executor. CeleryKubernetes Executor ‚Äî It allows users to run a CeleryExecutor and a KubernetesExecutor simultaneously. An executor is chosen to run a task based on the task‚Äôs queue. Choice of this executor is only needed in a few cases . Dask Executor ‚Äî Dask clusters can be run on a single machine or remote networks. Hooks A high-level interface to establish a connection with databases or other external services. List of different available hooks What if something I‚Äôm interested in is not present in any of the modules? You didn‚Äôt find the right operator, executors, sensors or hooks? No worries, you can write your custom stuff. Airflow provides Base classes which we can inherit to write our custom classes. from airflow.models import BaseOperator from airflow.sensors.base import BaseSensorOperator from airflow.hooks.base_hook import BaseHook from airflow.utils.decorators import apply_defaults class MyCustomOperator ( BaseOperator ): @apply_defaults # for default parameters from DAG def __init__(**kwargs): super(MyCustomOperator).__init__(**kwargs) pass def execute(self, conext): # we will cover more about context in next part. #your logic pass class MyCustomSensor ( BaseSensorOperator ): @apply_defaults # for default parameters from DAG def __init__(**kwargs): super(MyCustomSensor).__init__(**kwargs) pass def poke(self, context): #your logic pass class MyCustomHook ( BaseHook ): @apply_defaults # for default parameters from DAG def __init__(**kwargs): super(MyCustomHook).__init__(**kwargs) pass def get_connection(self): #your logic pass Managing task dependencies Most of the time in a workflow, to execute a task, we need to check the prerequisites for that task to execute. It is called dependencies. There are different types Dependencies 1- Linear dependencies ‚Äî Each task must be completed before executing the next. For instance, Airflow representation first_task >> second_task >> third_task first_task must be completed before the second and third tasks. And second_task must be completed before third_task 2- Fan-In and Fan-Out dependencies ‚Äî In this dependency, a task waits for multiple tasks to finish. For example, Airflow representation first_task >> second_task >>third_task >>end_task extra_dependent_task >> third_task Branching Branching helps to take decisions to decide which task or dag to execute. 1- Branching within the Task Programmatically handled the task execution def _decide_task(**context): if context[\"execution_date\"] < ROLLOUT_DATE: old_task(**context) else new_task(**context) ... task_branching = PythonOperator( task_id=\"task_branching\", python_callable=_decide_task, ) Cons ‚Äî Difficult to know by the DAG visualization which path was executed and why 2- Branching within the DAG Dummy Operator helps to achieve branching within DAGS. Let‚Äôs create below DAG So above, we run our job daily and there are two datasets (old and new) that are required to be merged to reference data to supply input to combine_task task. So, here three datasets are being joined for data supply. This way, joined_data will start executing as soon as all of its parents have finished executing without any failures, allowing it to continue its execution after the branch However, to make it a bit better, we can add a dummy task to mark the completion of joining old and new datasets. It‚Äôs better to safeguard that old and new data are surely available for the next run. for example, from airflow.operators.dummy import DummyOperator join_branch = DummyOperator( task_id=\"join_old_and_new_data\", trigger_rule=\"none_failed\" ) Out workflow will look like this Now adding an extra task for the old and new datasets can help us to make it clear visual and also to enable control to make sure that data is available for the next run. Trigger Rule All Airflow operators provides trigger_rule argument the defines as trigger this task when all directly upstream tasks have succeeded All existing options are all_success: (default) all parents have succeeded all_failed: all parents are in a failed or upstream_failed state all_done: all parents are done with their execution one_failed: fires as soon as at least one parent has failed, it does not wait for all parents to be done one_success: fires as soon as at least one parent succeeds, it does not wait for all parents to be done none_failed: all parents have not failed (failed or upstream_failed) i.e. all parents have succeeded or been skipped none_skipped: no parent is in a skipped state, i.e. all parents are in a success, failed, or upstream_failed state dummy: dependencies are just for show, trigger at will Cross Communication aka XCOM XCOM is used to share data between the tasks. xcom_push - register data in Airflow Metadata db xcom_pull - use registered data from Airflow Metadata db example ‚Äî Pay attention to the use of task instance (ti) within jinja-template to push and pull the variable. from airflow import DAG from airflow.utils.dates import days_ago from airflow.operators.bash_operator import BashOperator default_args = { 'owner': 'airflow', 'start_date': days_ago(1), } dag = DAG('xcom_example', schedule_interval=None, default_args=args) task_1 = BashOperator(task_id=\"task_1\", bash_command='echo \"{{ ti.xcom_push(key=\"xcom-key\", value=\"xcom-data-to-share\") }}\"', dag=dag) task_2 = BashOperator(task_id=\"task_2\", bash_command='echo \"{{ ti.xcom_pull(key=\"xcom-key\") }}\"', dag=dag) task_1 >> task_2 Limitations Don‚Äôt use it to share BIG DATA ( Be mindful that Airflow is a orchestrator not computational framework) Any value stored by an XCom needs to support being serialized. Accessing Metadata Database Airflow Metadata database can be used for storing DAG configurations, tables, constants and IDs. It uses Key-Value pair to maintain these variables. Stores configuration (variables) can be accessed by Variable . # Suppose we have kept dag_config in metadata database in this format # dag_config = {\"key1\":\"value1\", \"key2\":[\"a\", \"b\", \"c\"]} # then below is the way to retrive them in the dag get_dag_config = Variable.get(\"dag_config\", deserialize_json=True) config1 = get_dag_config[\"key1\"] config2 = get_dag_config[\"key2\"] # If variable is simply saved in this format key1 = value1, then we use get_key1 = Variable.get(\"key1\") # Similarly we can set the variables in metadata database. Variable.set(\"my_key\", \"my_value\") Accessing via command line #import variable json file docker-compose run --rm webserver airflow variables --import /usr/local/airflow/dags/config/dag_config.json # get value of key1 docker-compose run --rm webserver airflow variables --get key1 Testing For Airflow, generally below tests can be performed. DAG Validation Test ‚Äî To test whether DAG is Valid and Acyclic. Unit Test ‚Äî To test python functions, operators etc. Integration Test ‚Äî To test whether tasks of the workflow can connect each other. Workflow Test ‚Äî To test complete pipeline Data Quality Test Check these blogs for in depth knowledge. medium-blog airflow-testing-example Best Practices Write a clean DAG and stick with either of the one ways to create your DAG (with a context manager or without context manager). Stick to a better naming convention while writing your task name. Be explicit and logical. Keep computation code (SQL, script, python code etc.) and DAG definition separate. Every time DAG loads, it recomputes, hence more time it took to load. Don‚Äôt hardcode either constant value or any sensitive connection information in the code. Manage it in the config file or at the central level in a secure way. Create the tag and use it for a quick look to group the tasks in monitoring. Always search for existing inbuilt airflow operators, hooks or sensors before creating your custom stuff. XCOM is not for heavy data transfer. Data Quality and Testing often get overlooked. So make sure you use a standard for your codebase. Follow load strategies ‚Äî incremental, scd types in your code to avoid unnecessary data load. If possible, create a framework for DAG generations. A meta wrapper. Check out this repo . Specify configuration details once ‚Äî The place where SQL templates are is configured as an Airflow Variable and looked up as a global parameter when the DAG is instantiated. Pool your resources: All task instances in the DAG use a pooled connection to the DWH by specifying the pool parameter. Manage login details in one place ‚Äî Connection settings are maintained in the Admin menu. Know when to start a task. Normal scheduler or Event Trigger? Where to go from here? An essential part of learning anything is to create a project. When we build something, we understand the working principle and its terminologies the hard way. We have touched almost everything we need to work on any Airflow project. However, you might need to come back here again to refresh the theory or google some advanced topics once we start working on the project (for example, dynamic dag, task creation ‚Äî dag factory, deployment, monitoring, dag management, data quality checks, unit tests, etc). Do let me know if you want to see more details on those topics. But for now, we are set to go. So, let‚Äôs create a project and put our learning into action üí™ Photo by Kevin Jarrett on Unsplash Check out below GitHub repos for the Blog and hands on project. Project ‚Äî Airflow ‚Äî Chapel Hill Survey Data Analysis blog ‚Äî Airflow-Notes Reference The airflow community is very active, and many contributors across the globe are enriching and creating several packages and utilities to make developers‚Äô life easy. It is always good to follow the community and people on Twitter and GitHub to be in touch with new releases. Most of the details in these notes are taken from the below links. Check it out for more information. Data Pipelines with Apache Airflow (Highly recommended) Source code (Apache Airflow github) Documentation (official website) Confluence page Slack workspace Astronomer-blogs Thank you for reading üíï ! Orchestration with Apache Airflow ‚Äî A beginner‚Äôs guide for project-based learning !! was originally published in Analytics Vidhya on Medium, where people are continuing the conversation by highlighting and responding to this story.\n\nPhoto by Tolga Ulkan on Unsplash In one of my article s, I have described why a machine learning (ML) project is hard and also explained how a model can be managed by using MLFlow. I have already explained pretty much about model management in that blog but here I will go through with life cycle of the data for ML applications. The paradigm of ML is different with respect to traditional software development. Generally in traditional software, we write rules in code (our favorite programming languages) that act on data to give expected results but in Machine Learning it is governed by data. Data and answers get fed as input and the used algorithm predicts output (rules). Predicted outcomes solely depend on supplied data. If data is wrong in any means, the algorithm can‚Äôt perform as expected because of garbage in and garbage out principles. In Machine learning, data is crucial which was also recognized by Andrew Ng and he launched general awareness and campaign towards a data-centric approach for improving the model‚Äôs performance because improving data creates a marginally better impact on performance than hyper-parameterization. He literally emphasizes putting more focus on Data to improve the model‚Äôs performance and accuracy by being more data-centric rather than model-centric. In the Data-centric approach, data is the first-class citizen. But only having data is not enough unless it is curated and managed well. Data life cycle comes into the picture to address data quality, data management along with its traversal from generation, consumption, and prediction. Data life cycle focuses on various challenges of capturing, storing, processing, analyzing, and curating data used for all data applications including ML. It also focuses on industry-oriented tools and technologies to facilitate different services across end-to-end data flow by mitigating data risk, data quality, data consistency, and manual processes. The below image conceptualizes the data flow and its different stages along with a feedback loop to verify whether supplied data is performing and good enough as expected. Going further we will touch on a quick definition and function of each stage. Typical data flow in an ML/AI project Data Generation In this digital world, every moment we are generating a digital footprint. One of the Forbes articles says every day 2.5 quintillion bytes of data are being generated. In the future, it will only be accelerated with the further growth of the Internet of Things, Social Media, and other digital innovation. Data Collection There is too much data is being generated. During data collection, we focus on data that is complete and statistically significant. We also deal with whether we should capture all data or some time-sliced data to train our model and many times it depends on business choices and other practical reasons. Before start collecting data, we should ask ourselves certain relevant questions which can help us to figure out the right tools, technologies, and platform ‚Äî Are we interested in all data or a subset of the data? what patterns are we going to follow? Can we tolerate the loose data points? Is this data sufficiently accurate and reliable? How can stakeholders get access to this data? What features can be made available by combining multiple sources of data? Will this data be available in real-time? How much will this process cost in terms of time and resources? How will data be updated once the model is deployed? Will the use of the model itself reduce the representativeness of the data? Is there personally identifiable information (PII) that must be hide or anonymized? Are there features, such as gender, that legally cannot be used in this business context? Data Processing Data Processing is always slower than Generation and consumption as it requires validations and transformations before it can be consumed for any meaningful insights. A numerical representation of the several different features and feeding them into the model for the training are resource-intensive and iterative processes. In order to make data processing and algorithm fast and efficient, we need to first understand what kind of approach we need. All cores on a single machine Using a GPU Distributed computing e.g. MLib and Spark. Each approach has its own pros and cons. It is also worth noting that not everything can be parallelized. For example, data preprocessing and transformation can be done in parallel but feeding preprocessed data to algorithm depends on the type of algorithm. Some algorithms do, some don‚Äôt. However, for parallel processing, these tools can be useful (below is not the full list of all parallel frameworks but some popular ones) Spark & MLib FairScale ‚Äî A Pytorch extension. Elephas ‚Äî An extension of Keras . TensorFlowOnSpark ‚Äî TensorFlow deep learning framework with Apache Spark and Apache Hadoop . It enables both distributed TensorFlow training and inferencing on Spark clusters. Mesh TensorFlow ‚Äî Mesh TensorFlow (mtf) is a language for distributed deep learning, capable of specifying a broad class of distributed tensor computations. Data Storage There are a variety of data exists and as per business use cases, different varieties and volumes of data are required to train the model. These data could be structured (tabular data), unstructured (images, audios, videos) from huge volume to low volume, and for it, the definitely relational database could not be a good fit because of its tremendous cost. For the managed cloud, these storages exist which provide cheap cost and data replication to avoid any data loss e.g. SaaS Storage ‚Äî Azure BLOB, Azure, ADLS), S3 (AWS), and IaaS storages ‚Äî HDFS( Hadoop distributed file system). Data Management As I already mentioned Machine learning learns and predicts as per supplied data and its features. If data and features get changed so does the model‚Äôs output. And, we can‚Äôt afford this behavior if our model is in production. So, maintaining a version of the data and identifying the right source, data quality, and overall governance is important and that comes under the umbrella of Data Management. Data Management helps reproducibility for the auditing, debugging, tracing, and also implementing new changes by looking at all past behavior. This is quite a big topic and if someone is more interested to learn in more detail then t his research paper could be useful. Data Analysis Data Analysis is the heart of data science. We often hear the statement that ‚Äúdata scientists spend 80% of their time on data preparation‚Äù and this statement pretty much summed up the process of data analysis. Without it, we cannot confirm and conclude the features and their impact on the model. And for the same reason data scientists spend 80% of their time on getting data, cleaning data, aggregating data, reshaping data, and exploring data using exploratory data analysis and data visualization. A proper data analysis can tell us about all the good and bad about the data. Statistical techniques are used for data analysis to recognize hidden patterns, new features, data completeness. This step is also crucial to establish confidence in the model. Data visualization is also part of data analysis and it gets used within and as a product as well. Model Training Finally, we have reached the level where we do train our model with the data which we have gathered and curated so far. At this point, it is also important to evaluate the performance of the model. In general, we use different ensemble techniques and try to find the best combination of parameters and hyper-parameters. But in a Data-Centric approach , it is important to validate input data, features, and most importantly correct labels. The precision and accuracy of the model depend on this question ‚Äî Do we have quality (and labeled) data for our model? If not then we need to first focus on this before we proceed further. This process must start at the very beginning after data collection and by having subject/domain experts we can solve this problem. Below points can be taken as general guidelines for improving data quality (proposed by Andrew Ng in one of his courses ) For Small data- Clean labels are critical. Manual checks can be made through a dataset and fix labels. Can be set a rule to be agreed on for data labelers. For Big Data ‚Äî Emphasis data process. Big data can also have a long tail of rare events which eventually turn out to be small data challenges within big data. For Unstructured data ‚Äî May or may not have a huge collection of unlabeled examples. Humans can label more data. Data augmentation/synthetic data generation is more likely to be useful. For Structured data ‚Äî More difficult to obtain more data. ML models typically require at least 100 examples for each class to learn to classify that class. So if you have a large number of classes e.g. 500, you already need at least 50,000 examples. The data collection can be especially difficult when some of the classes are rare. When you have a large number of of classes, it‚Äôs likely that some of them are rare. Human labeling may be difficult for some use cases where we need expert domain knowledge and skills to determine actual ground truth. Interpretation At this level, we need an interpretation of the features and model. It‚Äôs a crucial step and to make it more reliable we need to understand bias, the behavior of the data, and AI ethics. The Explainability of data along with the model is very important and if not done in the right way it can badly impact the decision-making process. By having understandable and healthy dialogues between developers and users a common ground can be set to make data and models explainable. For instance, these questions are helpful for both parties For a decision/recommendation, a user might ask Why did you do that? Why not something else? When do you succeed or fail? How can I trust you? How do I correct an error? And if a dialogue (explainability of the model and reasoning for any of the above questions) is already established then those questions can‚Äôt bother the users. To understand more about interpretability please refer to this blog . Data Orchestrations We have covered all stages separately but in order to function altogether, they must need to communicate with each other. These stages need an orchestration tool that can glue and execute them together. Fortunately, there are many such tools that exist, and depending on the platform we choose for our application we can decide which fits best. Reference of few of them are here Apache Airflow Kubeflow Kedro Azure Machine learning Amazon SageMaker Monitoring and Feedback loop This is the step that verifies and continuously checks the relevance of a model. Model monitoring tools give us visibility into what is happening in production and also enable us to take appropriate action to improve the feedback loop. Centralize logging and Monitoring help us to track the performance and take corrective measures for the model by measuring data drift, concept drift, and other parameters /hyper-parameters. Ease of integration, monitoring functionality, alerting, and cost could be the parameters for choosing the right tools. A few examples are below. Evidently . Mlflow Azure Monitor and Azure Machine Learning suits AWS SageMaker Fiddler Challenges in Data Life Cycle The data flow from its generation to model training and other related metadata generation in between these processes required good management. If not managed properly it would be difficult to reproduce the model and build trust in it. Below are typical challenges that need attention on the data journey. Absence of Good Quality Data In the era of big data where Peta and Zeta bytes of the data are being generated and having good quality data is still a challenge. This fact was recognized and the Data-Centric approach was introduced by Andrew Ng. He proposed to focus on labeling (input-output data mapping) and quality of the data rather than big data. Data Quality, Domain and Subject Matter Experts, and Data Stewards are the people who can guide organizations to produce quality data for their analytics and decision-making teams. Data labeling is another challenge that falls under the Data Quality domain. Depending on whether data is small/big and structured/unstructured, human labeler and synthetic data augmentation/generation tool could be useful for such scenario. Data Drift Data drift is the phenomenon where prediction starts to behave in an unexpected way (there is also concept drift ‚Äî when the statistical property of target gets changed) because of the underlined data and features change. There could be two primary reasons for data drift Sample selection bias, where the training sample is not representative of the population. For instance, building a model to assess the effectiveness of a hiring program will be biased if the decision tends towards a certain group and ethnicity. Non-stationary environment, where training data collected from the source population does not represent the target population. This often happens for time-dependent tasks ‚Äî such as forecasting use cases ‚Äî with strong seasonality effects, where learning a model over a given month won‚Äôt generalize to another month. In order to avoid this situation, we need a better monitoring system to track supplied data for the model. Regular checks on data with univariate statistical tests come into effect to tackle this kind of situation. Data Security and Privacy Security and privacy comprise confidentiality, integrity, availability, authentication, encryption, data masking, and access control. These all situations are tough to control when data flows from several steps where generally we use a mix of tools and services. To control such situations in a better way multiple data privacy laws have been introduced to make sure that data get used in a legit way. Though following all these laws still much rely on the organization that uses personal data and to create a better world we need to follow these laws and data ethics. Data Sharing Data sharing is important in terms of accessibility. All components and modules of the system use data-sharing technologies. The purposes of data life cycle management are to re-use the data for various purposes and it should be shared among modules in the application. Therefore, every component in the system monitors different data (structured, unstructured, and semi-structured data). It passes through different locations, systems, operating atmosphere, and access by the different platforms. It helps to support to provide meaningful data at the right time. Metadata Management Metadata Management is a process for the organization to become truly data-driven. Effective metadata enables data to be discovered by users, systems, or AI/ML applications, whereas without it, a manual and time-consuming process is required to physically interpret whatever data is available and decide if it‚Äôs relevant or not. This also enables us to take timely action before the model starts degrading as we can fix the issue by monitoring metadata of the data and workflow. Besides that, it is helpful in audit, the provenance of the facts, comparing the performance of different artifacts, and helping in reproducibility. However, managing metadata from end to end is a tough job and if it is done in the right way it can enhance the capability of the organization to further perform analytics over its own data. Data Lineage It is an important part to track the data journey from generation to deployment including the feedback loop so that the pipeline can be upgraded and leveraged for optimal performance. Pretty much every organization struggles with this. Some are maintaining it partially and some are on their way to deal with it. However, there are a few tools e.g. Talend Data Catalog , IBM DataStage , Datameer , Spark Delta (a good option as open-source and also with a proprietary product known as Databricks Delta ), etc . that can be used but implementation is not always straight forwards because in this whole data life cycle we use different stacks to solve the problem and connecting each dot is a daunting task. Conclusion Data is the heart of an ML/AI application and handling data at each stage very much defines the success of all data applications. Practically, several challenges exist in implementation, and making a decision about relevant services and tools to create a robust data flow is a challenging task but if processes are designed and architected properly most of the challenges can be mitigated easily. The Global Data Management community is really a good place to refer for best practices regarding data management but we need to take all steps with a fine grain of salt as data operation for ML/AI is a bit different and rapidly evolving. Thank you for taking the time to read it üíï References https://www.sciencedirect.com/science/article/pii/S1877050920315465 https://www.youtube.com/watch?v=06-AZXmwHjo https://www.dama.org/cpages/books-referenced-in-dama-dmbok Book by Chip Huyen ‚Äî Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications Data Lifecycle of Machine Learning Project was originally published in Analytics Vidhya on Medium, where people are continuing the conversation by highlighting and responding to this story.\n\nPhoto by Wynand Uys on Unsplash In the real data world, the majority of the business problems get solved by ubiquitous relational databases and it is obviously a valid choice for several reasons. But with the advent of social media, machine data (IoT devices, PoS machines), and transactional data companies started to generate and consume large datasets. The Big Data term was coined to represent such unmanaged massive datasets. Solving business problems with traditional business intelligence tools especially the use of databases started to become a bottleneck as storing, managing, and computing such huge datasets required a cost-effective and fast processing solution. To solve these problems, Hadoop was created by Yahoo which referenced the concept of MapReduce a Google white paper. However, MapReduce had its own limitation for its conventional algorithms which were not designed around memory independence (It uses intermediate disk storage for its operation). Apart from that MapReduce and its related tools were mostly solving the problem of batch data processing and doesn‚Äôt have any support for ACID properties, real streaming data, and a good framework for data science to get insights for the big data. So, to solve MapReduce‚Äôs problem, Apache spark was introduced as a unified analytics engine for big data and machine learning. The fundamental computational principle of Apache Spark is in-memory processing. However, besides the initial advancement of Spark, there still few fundamental gaps were present which did not fully support modern data architectures to store, manage, and process big data. For example, if we talk about Data lakes which are defined as Storage that should be inexpensive and can have a variety of raw data in their native format (XML, JSON, CSV, ORC, PARQUET, etc) Can maintain historical dumps for these distinct formats. Historical data may require different types of data changes which further demand transactional operations on data. The above properties defined as flexibility of Data Lake can make it Data Swamp if not organized and managed properly. The same is true for all file-based architectures where data get extracted from Data Lakes. Data Lake also brings challenges that are needed to solve. Frequent refresh of metadata Handle schema changes and also enforce schema. Handling small size files(in case of streaming data ingestion and processing) Modification of existing data. Optimize performance Managing and re-sorting data by an index if data is spread across many files and partitioned by some columns. To keep the above challenges in mind, and not convert a precious data lake into a data swamp, Databricks launched a product named Delta Lake. It has been made open source and in this blog, we are going to see how is it useful and where can we leverage it in different data applications. Delta Lake Delta Lake is a file-based, open-source storage format that enables the organization to build modern Data Products to bring governance and structure of data to a single place and most importantly provide reliability on the data to ensure that insight generated by the organization is trustworthy. It solves major challenges highlighted above by adopting open-source file format (parquet) First-class support for Business intelligence, Machine Learning, and Data Science. Solve major challenges of data warehouse including a. Reliability ‚Äî Keeping the data lake and warehouse consistent b. Data Staleness ‚Äî Data in the warehouse is stale compared to the data lake as it gets fewer and infrequent updates. Source: https://databricks.com/product/data-lakehouse Now after reading the definition of Delta Lake a naive question might pop up ‚Ä¶well, it is clear that this is some storage system and intelligently manage the gaps which were not possible to fill before but how exactly does data get organized and pipelined by Delta Lake? This is achieved by following a Data Quality pattern from Data Ingestion to Data Consumption. As we move from Ingestion to Consumption, the quality of data keeps improving and getting better. To represent this idea, Delta Lake defined this data quality process into different layers which are called bronze, silver, and gold layers. Bronze: Keep data in as-is form (raw form e.g. JSON, Parquet, IOT data, XML, etc) Silver: More refined view of organization data. It can be query directly and data is clean, normalized, and can be considered as a single source of truth. Gold: This is aggregated data layer reserved for different business use-cases. From the Gold layer, reports, analytics can be built because that is the place where data is aggregated as per business use-case. Indeed above are fancy names but it deserves because in each layer data quality get improves and in the final stage we get data that can be used for analytics. To understand how the above challenges are addressed by Delta Lake, we need to deep dive into its key elements and features. Delta Lake Elements Delta lake has 4 main elements that act as a foundation to build Delta Lake features (features are described in the next section). Delta Files ‚Äî It uses Parquet Files which is a columnar file system optimized to store tabular data. Question: If it uses Parquet files, then how Delta is different? Answer: Indeed Delta uses parquet files for its storage but the only difference between the Parquet and Delta tables is the _delta_log folder which stores the Delta transaction log and helps to achieve different Delta features (will be discussed in the next section). 2. Delta Tables ‚Äî It is a collection of data which is consists of 3 things a) Data files containing data b) A Delta table registered in a Metastore which tracks the metadata. c) A transaction log with Delta files to keep track of all transactions. 3. Delta Optimization Engine ‚Äî Delta Engine accelerates data lake operations and supports a variety of workloads ranging from large-scale ETL processing to ad-hoc, interactive queries. Many of the optimizations get applied automatically. 4. Delta Lake Storage Layer ‚Äî A place where the organization keeps its data in object storage at a cheap cost and later Delta can be used to access these files. Delta Lake Features Compatible with Apache Spark API This is an important feature to switch the underlying storage and application smoothly to Delta Lake. Being compatible with Apache Spark makes it easy to leverage new features. This particular feature is useful for such projects which are developed on Spark but need migration to the Delta. Format required this flow to go into delta At file level Any file format -> parquet -> Delta At code level Little tweak as Delta built on the same Spark engine. Look at the below snippet- from delta.tables import DeltaTable parquet_table = \"some_parquet_table\" partitioning_scheme = \"some_id int\" DeltaTable.convertToDelta(spark, parquet_table, partitioning_scheme) Isn‚Äôt the above snippets look familiar like Spark code? ACID transaction All data changes written to Delta Storage are committed for durability and guarantee no more partial or corrupted files. It means the reader can read consistent data. To understand the ACID behavior, we need to unpack the transaction files which get maintained by Delta lake to facilitate this property. To check the transaction log, we can list the _delta_log folders where all transaction-related data get captured. Inside the folder _delta_log, we can see two files are created as .crc and .json with a big sequence of zero. If any operation happens on data then another .json and .crc files with incremental sequences get generated. It‚Äôs important to understand what content these files capture and how are they beneficial for several purposes. CRC (cyclic redundancy check) file helps spark to optimize its query as it provides key statistics for the data. On the other hand, JSON file captures a lot of info. Let's read the JSON file and list the columns info then we see there are 4 columns named add, commitInfo, metaData, and protocol . If further expand the add column we get the content confirming data change , modificationTime, path, size, and basic stats of the data. expanding add column expanding metaData column commitInfo ‚Äî Information on commits. protocol ‚Äî Enables new features by switching the Delta Lake transaction log to the newest software protocol (reader and writer version) Now let‚Äôs try to create a table from the written path and also try to update the records from the table. Data is updated for new id value. So, internally .json and .crc files have also been changed to capture this new data change. Let‚Äôs re-read _delta_log folder and its files What changes do we see now? We can see as the table is updated, another JSON & CRC file gets created. If we open the 00000000000000000001.json file and see the columns (did you notice something here? In file 00000000000000000000.json, column add was present because we added the records and now an extra column is added as remove to show which data is updated) and content then we can find all the information of the transactions which have been taken place for the dataset. So, for every operation, Delta internally maintains the files to captures its related info and this is where a lot of extra information gets maintained to take the decision to handle different operations and also enable all good features on Data which was not possible before for other file formats. Takeaway‚Äì Delta Lake Transaction Log is a single source of truth for our data. ACID also enables Delta to support upsert and merge . Schema on Read It is the concept where data is applied to a plan or schema when it is pulled out of a stored location. Before Delta, in Spark or Hadoop traditional architecture whenever a table was added with the additional dataset, every time to read all newly added datasets with old records, we required to MSCK REPAIR TABLE command to refresh the metastore of the backed file system. But, in delta this problem is resolved, and whenever the user query the data, is ensured to get the latest dataset. Data Versioning aka Time Travel Data versioning is a similar concept which we follow on code versioning (git or any other code management software). We can switch to a different version of the dataset based on the time or specific number(version). This feature simplifies building the pipelines for the dataset which keeps changing from time to time. Constantly changing data brings auditing challenges, reproducing experiments, fixing data mistakes, and a rollback to switch the desired state if required. How to use it? It is easy to select data for a specific version or timestamp from a delta table. .option(\"timestampAsOf\", timestamp_expr) => to get specific timestamp e.g. option(\"timestampAsOf\", \"2018‚Äì10‚Äì18T22:15:12.013Z\") .option(\"versionAsOf\", version) => to get specific version e.g. option(\"versionAsOf\", 9) It is also possible to roll back to a different version Schema enforcement or Schema Validation Checks schema validation of the data on write and helps to ensure the data correctness. It is a mandatory check to reject writes to a table that don‚Äôt match with the defined schema. These rules are followed to determine whether a write to a table is allowed or not Cannot contain any additional columns that are not present in the target table‚Äôs schema Cannot have column data types that differ from the column data types in the target table. Cannot contain column names that differ only by case ( Delta is case sensitive ‚Äî Delta and delta is not the same) Note ‚Äî This feature helps to validate a lot of things for which generally we create custom data validators in real projects. Just having a standard delta format is a rescue to avoid a lot of data issues. Schema Evolution It helps users to easily modify the table‚Äôs current schema to accommodate new data changing over time. General use of this feature is during an append and overwrite operation to automatically adapt the schema to include one or more new columns. How to use it? During the writing of the dataframe into a delta file, we need to set the mergeSchema property to true. df.write.format(\"delta\")\\ .mode(\"overwrite\")\\ .option(\"path\", \"file_path\")\\ .option(\" mergeSchema \", \" true \")\\ .partitionBy(\"partition_col\")\\ .saveAsTable(\"table_name\") It is also possible to retrieve Delta table history DESCRIBE HISTORY <table_name>-- get the full history of the table DESCRIBE HISTORY delta.<delta_file_path> Optimization So till now, we have seen that Delta files capture several statistics around the operating file. These statistics have been written by Delta lake creators for a purpose and one of the obvious purposes is to leverage the optimization and enhance the performance of Delta Lake. In this final section, we will learn some optimization techniques which can be used (if required) to improve the performance of the delta files/tables. a. Optimize by Compaction Optimize operation performs file compaction i.e. small files are compacted together into larger files (up to 1 GB). Delta doesn‚Äôt support OPTIMIZE as an automatic process but expects users to perform this based on need. How to use it? Optimize <table_name> OR delta.`delta file location` b. Z-Order This is a technique to collocate multidimensional data to one dimension by preserving the locality of the data points. In simple words ‚Äî it is a way to keep related information in the same set of files. Internally it creates fewer files to keep data and when a search or query gets fired it scan fewer files to get the result. Once we apply ZORDER on certain columns (cannot apply on partition column), Delta perform the below activities under the hood Takes existing parquet files within a partition. Maps the rows within the parquet files according to the selected column for ZORDER . In the case of only one column, the mapping above becomes a linear sort. Rewrites the sorted data into new parquet files. Question: How to choose ZORDER columns? Answer: The thumb rule is to select those columns which filter data with the largest set (low cardinal column but don‚Äôt choose partitioned columns). Multiple columns can also be selected but that reduces the effectiveness of locality. How to use it? OPTIMIZE table_identifier [WHERE clause] [ZORDER BY (col_name1, col_name2, ...)] -- be mindful not to use partition column. c. Partition Pruning It is used to speed up the query to minimize the amount of data read. Simple meaning always put partition column(s) in where clause to filter the data. d. Data Skipping Apply where clause to add more columns in the query to filter the records and give hints to delta files ( files that keep statistics) to skip unnecessary information. e. Vacuum Handy command to keep only relevant files and save the cost on the storage. How to use it? VACUUM <name-of-table> RETAIN <number-of-hours> I have tried to capture my understanding of the delta and I found it is important to understand its functionalities so that this can be leveraged fully for different use-cases. If we are working to build or design any analytical solutions (data lake, data warehouse, machine learning, handling streaming data, etc.) Delta can be safely chosen as standard storage for data processing. Thank you for reading üíï References This is an excellent book written by Delta lake creator if you want to further deep dive into it. https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf https://databricks.com/product/delta-lake-on-databricks https://docs.databricks.com/delta/optimizations/file-mgmt.html Demystifying Delta Lake was originally published in Analytics Vidhya on Medium, where people are continuing the conversation by highlighting and responding to this story.\n\nPhoto by Chad Kirchoff on Unsplash Have you ever run into a problem that your data structure is out of the memory to load all of your required intermediate data for the next processing and failed to maintain an internal state every time it is called? Does it sound familiar? ü§î Welcome To The World Of Generators Well, this kind of situation is pretty normal in programming and to avoid the same problem multiple solutions exist. But to tackle these kinds of the problem more elegantly, Python has specialized functions/expression which is typically known as Generator. In python wiki , it is defined as, ‚ÄúGenerator functions allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop.‚Äù So, Generator is nothing but a function/expression that is specialized to behave like an iterator . It means that Generator carries the properties of an iterator i.e. __iter__ and __next__ methods for a class. This is also called a generator pattern in software engineering. Now, let‚Äôs create the generator pattern for calculating a factorial in the following way, The above code gives us a nice structure to work with a generator pattern for factorial but its implementation is a bit verbose. Going further, the question pops up, Can we make it simpler? Let‚Äôs look at another snippet with less code but smarter implementation We see the code implementation is drastically reduced but it is still serving the same purpose in a much smarter way. Another interesting point of the above code snippet is yield . This behaves the same as a return statement in a function - but there is an important difference When the yield statement is encountered, Python returns whatever value yield specifies, but it pauses execution of the function. We can then call the same function again by next() and it will resume from where the last yield was encountered. Generators Vs Iterator image source: https://nvie.com/posts/iterators-vs-generators/ As we discussed above, we can sum up our understanding of what we have seen till now with the below points ‚òÖ A generator is a function/expression with an iterator capability (but the reverse is not true) that provides the implementation of a design known as a generator pattern. There are two types of generators in Python. Generator functions and Generator expressions . ‚úì A generator function is any function in which the keyword yield appears in its body. ‚úì A generator expression is the generic equivalent of a list comprehension. Its syntax is really elegant for a limited use case A list comprehension can be converted into a Generator by wrapping it within bracket () instead of the list[] See the use of () instead [] in the list comprehension ‚Äî line 4 ‚òÖ It is memory efficient as it doesn‚Äôt store everything at once but execute during the iteration and maintains the internal state. Use Case & Implementation Amazon Simple Queue Service (SQS) is a messaging system which use to hold the generated messages for a defined period. Any consumer wish to consume those hold messages can connect to SQS and process them. More information about AWS SQS can be found here . Now, let‚Äôs think if as a consumer, I‚Äôm asked to consume messages from SQS and process them to get a general summary of messages, e.g. summation of value based on type (once we will see the sample data and example then requirement will be quite clear) then what would be our implementation steps? To do so, maybe the below steps can be taken Connect to AWS SQS. Consume the hold messages and store them in some data structure for further processing. Perform analytics on captured messages. So far so good. But did you see any problem with the second bullet point? By consuming all of the messages in a data structure (e.g. in a list or array) at a certain point these placeholders can explode if there are huge messages in SQS. Out of Memory is an obvious and common problem at this stage. Agree?? Here the significance of the generator comes into the picture. Step 1- raw messages Let‚Äôs assume the sample datasets in SQS looks like {‚ÄòMessages‚Äô: [{‚ÄòMessageId‚Äô: ‚Äò762b5a79‚Äì29b2‚Äì72b8-f788‚Äì606ccf806629‚Äô, ‚ÄòReceiptHandle‚Äô: ‚Äòurgtrhwwtg‚Äô, ‚ÄòMD5OfBody‚Äô: ‚Äò91e9b5c6e0f9860130e56f575680744d‚Äô, ‚ÄòBody‚Äô: ‚Äò{‚Äútype‚Äù: ‚Äúpageview‚Äù, ‚Äúvalue‚Äù: 1, ‚Äúoccurred_at‚Äù: ‚Äú2021‚Äì03‚Äì03 10:33:38‚Äù}‚Äô , ‚ÄòAttributes‚Äô: {‚ÄòSenderId‚Äô: ‚ÄòAIDAIT2UOQQY3AUEKVGXU‚Äô, ‚ÄòSentTimestamp‚Äô: ‚Äò1614764020782‚Äô, ‚ÄòApproximateReceiveCount‚Äô: ‚Äò6‚Äô, ‚ÄòApproximateFirstReceiveTimestamp‚Äô: ‚Äò1614784965208‚Äô}}], ‚Ä¶‚Ä¶ up to n messages } Step 2- parsed messages Now, we are interested in the italic bold part from the above message to perform our analysis i.e. below string {‚Äútype‚Äù: ‚Äúpageview‚Äù, ‚Äúvalue‚Äù: 1, ‚Äúoccurred_at‚Äù: ‚Äú2021‚Äì03‚Äì22 10:33:38‚Äù}‚Äô, ‚Äò Body‚Äô: ‚Äò{‚Äútype‚Äù: ‚Äúpageview‚Äù, ‚Äúvalue‚Äù: 1.5, ‚Äúoccurred_at‚Äù: ‚Äú2021‚Äì03‚Äì22 10:33:38‚Äù}‚Äô , {‚Äútype‚Äù: ‚Äúdoc-view‚Äù, ‚Äúvalue‚Äù: 4.5, ‚Äúoccurred_at‚Äù: ‚Äú2021‚Äì03‚Äì22 10:33:38‚Äù}‚Äô ‚Ä¶‚Ä¶ up to n messages If we keep accumulating the above messages into the list then that won‚Äôt be ideal, because if our queue is large, this spends a lot of memory . We might run out of memory, and lose all the messages. But if we rewrite this as a generator , and yield the messages as we receive them then the code will become polished, optimized, and memory efficient. Step 3- type and its sum Once all messages are parsed then we need to get a sum for all the type and an expected output should look like this { ‚Äú pageview ‚Äù: 2.5,}, { ‚Äú doc-view ‚Äù: 4.5,} ‚Ä¶..up to k messages ( where k ‚â§ n) For this use-case implementation, I will use boto3 SDK. S Installation guide and configuration settings can be found here Let's see the implementation in the code. So, in this blog, we have learned generators, usage, and their implementation. To keep the explanation and implementation simple, just the bare minimum code is captured here. However, the full implementation including AWS SQS message deletion (after consumption), sum and counts stats on each type, logging (console and TimedRotatingFileHandler ‚Üí an important handler if your job is running for a long time) , writing stats in an out file, unit test (a sample test), setup conf and Dockerfile are developed as part of full sample project which can be found here on my github . Thanks for reading ‚ù§Ô∏è References https://aws.amazon.com/sqs/ https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sqs.html Optimize your code with Python Generator was originally published in Analytics Vidhya on Medium, where people are continuing the conversation by highlighting and responding to this story.\n\nPhoto from Shutterstock Ethics is an important aspect of life and unethical of anything is simply harmful and scary. The same principle is also valid and legitimate in the technical world. With the evolution of big data and high performant computing machines, artificial intelligence (AI) has been making a leap and bound progress. We know in order to make an efficient AI system we need well-curated data and an algorithm that can work for unseen data in a super performant way. So, DATA is the main fuel for an AI or ML (Machine Learning) algorithm, and collected data for these purposes can have several biases and unethical elements which can confuse or deviate an algorithm to behave differently and generate a whole unethical system which could be dangerous for the society. Targeted advertisement, society bias, fake news are some relevant examples but there are several other instances that happened in the past where an ML algorithm was misused (sometimes unintentionally because all possible behavior of a model was not tested) and proved to behave in an undesired way. Few examples of such use cases are below- UK‚Äôs Grading algorithm ‚Äî Recently, the UK Department of Education discarded grades generated by an algorithm designed to predict the performance of annual A(advance) Level qualification. This initiative was taken due to the COVID pandemic and the result predicted by the algorithm was downgraded by more than a third of A level results in the UK. The developed model primarily focussed on two features ‚Äòstudent‚Äôs past performance‚Äô and ‚Äòschool‚Äôs historical performance‚Äô to predict the grades of the students. The prediction of the algorithm went in favor of the private schools and the secondary selective and sixth form schools where teacher assessment used to be good severely impacted. Unethical facial recognition ‚Äî In the Washington Post article , it was published that how US‚Äôs Immigration and Customs Enforcement unethically collected a large volume of data to analyze day-to-day activities of immigrant communities. This is an example of the unethical use of AI to abuse the civil rights of targeted communities. Amazon‚Äôs AI recruiting tool ‚Äî The developed tool for hiring by Amazon started to bias against female job applicants. See the full story here Unemployment benefit Fraud ‚Äî MiDAS (Michigan Integrated Data Automation System) an unemployment system that was launched to replace its old COBOL-based legacy system booked many people for fraud to claim unemployment benefits. It wrongly accused at least 20,000 claimants of fraud, a shockingly high error rate of 93 percent. The problem was the alleged ‚ÄúRobo-Adjudication‚Äù system, which lacked human oversight. The application seeks out discrepancies in claimants‚Äô files and if it finds one, the individuals automatically receive a financial penalty, and then, they‚Äôre flagged for fraud. Have a look at this metrotimes post for more details. Microsoft‚Äôs unveiled Tay ‚Äî A Twitter bot launched with the idea of ‚ÄúThe more you chat with Tay, the smarter it gets‚Äù got corrupted within 24 hours from its launch with the supply of all misogynistic, racist messages from Twitter. Check this post. Google‚Äôs hate speech detector ‚Äî Google‚Äôs AI tool developed to catch hate speeches started to behave differently towards black people (bias effect). So, looking at those malfunctioned AI/ML tools which were certainly developed by top developers and envisioned by great business leaders, suddenly became threats to society. And then the real question appears how can one create an ethical way of working and sensible responsibilities among all groups of collaborators (Data collector, Developer, Decision maker, Sales, Marketing, Executives, etc.)? Several papers have been published in this direction and there are no golden rules to be followed religiously but few important aspects of this problem can be summarized. I would like to highlight them as part and purpose of this article. The 5 Cs Data Ethics ‚Äî 5 Cs a) Consent ‚Äî An agreed consent between the data provider and data service. b) Clarity ‚Äî Clarity is directly related to consent to tell data providers that what are they providing. c) Consistency & Trust ‚Äî Unpredictable person cannot be trusted hence trust requires consistency. These facts are important and should be the part of data ethics as we have seen many security incidents where these things were broken explicitly or implicitly. Yahoo, Target, Anthem, local hospitals, and government data are a few examples of this. d) Control & Transparency ‚Äî Once the consent is provided now it becomes important to understand how does the data is being used. Do users have any control over them? These questions have important aspect because we know how big companies generally use public data for their own target advertising and creating political and religious sentiments. To address these things up to a certain extent, Europe‚Äôs General Data Protection Regulation (GDPR) is a good example that enables users to give their consent to remove the data from the system where it was submitted earlier. e) Consequences ‚Äî Risk can never be eliminated completely. The product using AI and ML gets builds and sometimes due to potential issues around the use of the data some unforeseen consequences arrive. Many regulations and guidelines have been formed to tackle these kinds of problems, e.g., Children‚Äôs Online Privacy Protection Act (COPPA) to protect children and their data and Genetic Information Nondiscrimination Act (GINA) in response to rising fears that genetic testing could be used against a person or their family. Implementing 5 Cs is not the individual responsibility but it requires the entire team with the idea of shared responsibilities. 2) Biased Data or Biased Algorithm ‚Äî This is often an arguable topic among data practitioners for the root cause of bias in the real world of AI. Is it Data or Algorithm? And of course, there are different views but in most of the case, it is human who developed these mathematical model or feed them with datasets which are often created or collected by them. So, ultimately biased is somewhere related to humans where we need to show the responsibilities and best practices while collecting the data or designing a sci-fi AI model. 3) Context ‚Äî Contextual awareness plays a significant role for anyone working in AI and ML areas. Understanding the data to answer some standard questions like what and why am I trying to achieve certain things helps to design an algorithm which senses the decision with the context of the data. 4) Model Fairness & Explainability ‚Äî Can a model‚Äôs result be trusted? Are they explain why particular features and their weight is important for predictable values? Can we explain that? These questions are relevant to decide that developed models are fair to use for and they are justifiable enough for the purpose they have been developed. 5) Model Drift ‚Äî The analytical models need to be revised with time otherwise there is a high chance of instability and erroneous predictions. In ML/AI, this behavior is defined as Model Drift. It has been classified into two broad categories. i) Concept Drift -It means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. ii) Data Drift - This happens when the statistical properties of the predictors change(independent variables). These changes can bound the model to fail. The classic example of data drift is seasonality in data. Black Friday time period always records good sell than other times of the year. 6) Ethics and Security training ‚Äî Theory learned or taught as part of the educational curriculum lacks practical implementation and that‚Äôs why training of ethics and security is important for professional people because it enables them to implement these principles in the related field. So, if we collect those points as a checklist and follow them while making any decision, then that could be helpful to avoid common mistakes. These points can also enable us to become more responsible and sensitive towards our work. Mike Loukides, DJ Patil, Hilary Mason have compiled the below checklists in their book Ethics and Data Science and it is worth having them in our data product checklist. Checklist ‚Äî Have we listed how this technology can be attacked or abused? Have we tested our training data to ensure it is fair and representative? Have we studied and understood possible sources of bias in our data? Does our team reflect the diversity of opinions, backgrounds, and kinds of thought? What kind of user consent do we need to collect to use the data? Do we have a mechanism for gathering consent from users? Have we explained clearly what users are consenting to? Do we have a mechanism for redress if people are harmed by the results? Can we shut down this software in production if it is behaving badly? Have we tested for fairness with respect to different user groups? Have we tested for disparate error rates among different user groups? Do we test and monitor for model drift to ensure our software remains fair overtime? Do we have a plan to protect and secure user data? So, in short data ethics principles can help to leverage the full benefit of AI for the good cause of society without any fear and also can create a sense of responsibility among all participants who develop data products to solve critical problems. References ‚Äî https://hub.packtpub.com/machine-learning-ethics-what-you-need-to-know-and-what-you-can-do/#:~:text=Ethics%20needs%20to%20be%20seen,and%20building%20machine%20learning%20systems.&text=But%20by%20focusing%20on%20machine,robust%20and%20have%20better%20outcomes. https://learning.oreilly.com/library/view/ethics-and-data/9781492043898/ https://www.bbc.com/news/explainers-53807730 Data Ethics in Artificial Intelligence & Machine Learning was originally published in Analytics Vidhya on Medium, where people are continuing the conversation by highlighting and responding to this story.\n\nPhoto by Greg Ortega on Unsplash The life cycle of a machine learning project is complex. In the paper Hidden Technical Debt in Machine Learning Systems , Google took the reference of the software engineering framework of technical debt and explained that the maintenance of real-world ML systems can incur massive costs. The below image truly depicts the real scenario. The sandwiched tiny black box, surrounded by big boxes is the Magic Machine learning Code :) and to run this magic code in the production, we need to deal with several other processes e.g. Data collection, verification, feature extraction/generation, process management, deployment, serving infrastructure, monitoring, etc. Apart from that when the ML system is in the exploration phase, a team of data scientists/ML engineers keep close eyes on the metrics and performance of the different models to get an optimized one. Capturing and sharing these metrics and analysis with other teams or following up with businesses to share the model requires a robust model lineage system (storage, versioning, reproducibility). Beyond this, once the value of the model is proved then further it requires several toolings including a computational and deployment framework to support the model‚Äôs execution in production. If the performance of the model degrades then it also needs to be tracked timely and re-trained accordingly with the changed dataset. And this whole process makes the complete life cycle of an ML project more complex than the software development life cycle. The difference between traditional Software and Machine learning development can be summarized as follows. Keeping those facts in mind, many enterprises created their own platform to support the full life cycle of analytical model development but again it takes a toll to maintain an efficient and dedicated engineering & platform team. Examples of such platforms are Google‚Äôs Tensorflow, Facebook‚Äôs FBLearner, and Uber‚Äôs Michaelangelo. But for these platforms as well a few challenges exist in the form of Limited or small sets of Algorithm support. Non-sharable code. Tightly coupled with enterprise infrastructure which generally doesn‚Äôt offer all need. Therefore to solve all the above problems, Databricks open-sourced the library named MLflow . The objectives of MLflow not only support a complex ML life cycle but also provide user-friendly API to mitigate common challenges like model reproducibility, sharable artifacts, and cross-language supports. MLflow As per the MLflow documentation MLflow is an open-source platform for managing the end-to-end machine learning lifecycle . The design philosophy of MLflow is modular and API based. Its functionality is divided into 4 parts Tracking Projects Models Registry source: databricks.com/mlflow Let‚Äôs understand the above individual components in the details and later we will see the implementation of the same. 1. Tracking MLflow tracking is a meta-store of MLflow and a centralized place to get the details of the model. It uses HTTP protocol to establish a connection between the client application and the tracking server. The tracking server captures the below details for the model and uses backend stores to log Entity and Artifacts. Logging parameters Code versions Metrics Artifacts (Model and data files) Start and End time of the run Tags and notes as additional information By default MLflow tracking backend stores uses local file system and create mlruns directory to capture entities and artifacts. The file structure of mlruns folder For the production use case, MLflow provides different storage options to store artifacts and metadata. Artifacts -> Amazon S3, Azure Blob, Google Cloud Storage, Databricks DBFS Metadata-> SQL store(PostgresSQL, MySQL, SQL Lite, SQL Server etc), MLflow plugins Schema for customized entity metastore etc. 2. Projects MLflow Project is nothing but an organized and packaged code to support the reproducibility of a model. To organize the project‚Äôs file and folder, MLflow comes up with a file named MLproject (a YAML file) which can be configured as per the Data science project requirement. In MLproject we can also configure the docker container and Kubernetes for project execution. It also provides command-line tools and API to execute the project and create the workflow. A typical example of MLproject would be like this name : sklearn-demo conda_env : conda.yaml entry_points : model_run : parameters : max_depth : int max_leaf_nodes : { type : int, default : 32} model_name : { type : string, default : \"tree-classification\" } run_origin : { type : string, default : \"default\" } command : \"python model_run.py -r {max_depth} {max_leaf_nodes} {model_name}\" In the above example, a conda environment is defined as conda.yaml which is responsible to set the dependencies for the project. Steps to building an MLflow project 1- Create an MLproject file [define the entry point of the project] 2- Create a conda.yaml file for all python dependencies. 3- Create a python project and keep MLproject and conda.yaml file in the root directory ( or any other location where the main executor is kept) 4- Push the python project to GitHub 5- Test the local project as well as git hub local test ‚Üí mlflow run . -P <param> github test ‚Üí mlflow run git://<project-url> <param> MLproject can have a multistep project covering different entry points (production work-flow). The example of the same can be found at multiple-step-example. 3. Models The Models define a convention to save an ML model in different ‚Äúflavors‚Äù . As per the documentation Flavors are the key concept that makes MLflow Models powerful: they are a convention that deployment tools can use to understand the model, which makes it possible to write tools that work with models from any ML library without having to integrate each tool with each library In other words, the purposes of the Flavor are - To utilize the same memory format for different system To avoid the overhead of cross-system communication (serialization and deserialization) To provide common shareable functionalities Flavors are generally of two types ‚Äì In-built flavors (available for all popular Machine Learning algorithm and libraries) Custom flavors Below libraries are available as in-built flavors but also can be wrapped under custom flavors using python_function. H2O Keras MLeap PyTorch Scikit-Learn MLlib Tensorflow ONNX (Open Neural Network Exchange) MXNET gluon XGBoost LightGBM Custom Flavor It is possible to create a custom flavor for the model. Documentation to create a python custom flavor. Once the mlflow project gets executed, in the artifacts folder the MLmodel file gets created. Below is the example of the same for python_function flavor. artifact_path: decision-tree-classifier flavors: python_function: data: model.pkl env: conda.yaml loader_module: mlflow.sklearn python_version: 3.6.5 sklearn: pickled_model: model.pkl serialization_format: cloudpickle sklearn_version: 0.23.1 run_id: 10c75a05fb124eddbf2b13b458e9a26e utc_time_created: '2020-06-19 11:53:55.328301' 4. Model Registry The ML model management is a common problem in a large organization. To solve the challenges around model management, the model registry component was built. MLflow Model registry component manages the full life cycle of the machine learning model and provides Centralized model store: Storage for the registered model. Model lineage: experiment and run details Model versioning: Keep track of versions of the registered model. Model Stage: Assigned pre-set or custom stages to each model version, like ‚ÄúStaging‚Äù and ‚ÄúProduction‚Äù to represent the lifecycle of a model. Before deploying a model to a production application, it is often best practice to test it in a staging environment. This link is helpful to understand the workflow of the model stage transition. CRUD operations on registered models: Create, update, delete, archiving, listing, operations on models. Building an MLflow project from scratch The complete code can be found on my github . In the GitHub, I have added a mlflow-demo project demonstrating a scikit-learn and Keras model. However, in this walkthrough, I will demonstrate the scikit-learn project and its execution. The demonstrating project sklearn-demo has the below structure. The structure of the project can be re-arranged and reconfigured as per use-case requirements. This is just an example. Prerequisite- To replicate the example below prerequisite are essential. However, if the conda environment is set and dependencies are mentioned then it automatically creates an environment to execute the mlflow project. Required python version should be 3.5+ install mlflow, numpy, pandas, scikit-learn, scikit-plot, matplotlib, seaborn. Step 1- create conda.yaml & MLproject.yaml file for the project setup. These two files are an important aspect of mlflow‚Äôs Project component and consist of workflows executions such as entry points, the command to execute, dependencies, etc. conda.yaml name : sklearn-demo channels : - defaults dependencies : - python=3.7.6 - pip : - mlflow==1.8.0 - numpy=1.18.5 - pandas=1.0.4 - scikit-learn==0.23.1 - scikit-plot==0.3.7 - matplotlib==3.2.1 - seaborn==0.10.1 conda.yaml file is pretty simple to understand. name is the project name, dependencies are the version of python and pip list down all libraries which are required to execute the project. MLproject.yaml name : sklearn-demo conda_env : conda.yaml entry_points : model_run : parameters : max_depth : int max_leaf_nodes : { type : int, default : 32} model_name : { type : string, default : \"tree-classification\" } run_origin : { type : string, default : \"default\" } command : \"python model_run.py -r {max_depth} {max_leaf_nodes} {model_name}\" MLproject is also easy to comprehend. name: Any project name conda_env: It is the name of condo YAML file( it should have the same above-defined conda file name) entry_points: An important key in the file. It the execution point for the code. In the parameters section, we can define all command line params which we required to pass for the main script to execute. We can set the default as well as rely on the users for that. The name of model_run could be anything as per project setup and the default is main. So, our project is set to proceed further. Step 2- create a python file(prediction.py) inside models module (it can be created anywhere; just to make it modular, I have kept it inside models). https://medium.com/media/112f06d151cb76686a542f5b95f4e284/href The code is fairly simple and let‚Äôs understand it step by step A class TreeModel is created which is a wrapper class for the DecisionTreeClassifier model. The class has one class method named create_instance which accepts the parameter to create an instance of DecisionTreeClassifier. TreeModel also has 3 attributes named data, model, and params which gives dataset, model, and load parameters for the classifier. So, the important method in the TreeModel class is mlflow_run. This method is doing a lot of things for us to capture the artifacts, metrics, and plots. Using python context is also important to capture all required metrics in one go. These methods are important to understand and can be used with all kinds of models. mlflow.log_param(key, value): Capture a parameter under the current run. If no run is active, this method will create a new active run. mlflow.log_params(params): Log a batch of params(dictionary of param_name) for the current run. If no run is active, this method will create a new active run. mlflow.log_metric(key, value, step=None): Log a metric of the model for the given run. If no run is active, this method will create a new active run. mlflow.log_metrics(metrics, step=None): Log multiple metrics for the current run mlflow.log_artifact(local_path, artifacts_path=None): Log a local file or directory as an artifact of the currently active run. If no run is active, this method will create a new active run. mlflow.log_artifacts(local_path, artifacts_path=None): Log a local file or directory as an artifact of the currently active run. If no run is active, this method will create a new active run. Another utils module is there which has some general functions(to plot confusion matrix and roc) used across the project. Create a utils.py file with the below content. https://medium.com/media/fe9ca54d7761b8e7c317f4ba62de7d3d/href Step 3- In this step, we will create our main executor (model_run.py) which is going to acts as a driver for mlflow entries. https://medium.com/media/de8f4e556b646223c04f58855283fd39/href model_run accepts max_depth hyper-parameters and execute TreeModel. That‚Äôs it. Everything is set and we are good to go !!! Now to see the logged details by mlflow either we can execute our program directly as python model_run.py or mlflow run . -P max_depth=5 from the command prompt. Let‚Äôs see how mlflow has captured all the artifacts and metadata. There are many ways to check the logged details. Locally mlflow captures all artifacts, lineage, and metrics inside the mlruns folder (I have demonstrated and attached the screenshots at the beginning of the tutorial while explaining the theories). But mlflow also comes with handy UI command which enriches the user experience. Go to the command prompt, navigate to the mlruns folder and type mlflow ui a local browser http://kubernetes.docker.internal:5000 will prompt and by clicking on that we can explore all and useful details about the ML run. Home page of local MLflow. Here each run (individual execution of the code which generates folders and files) and experiment (name of the group of runs) gets registered. So let‚Äôs check the first run. Click the first run and the below page will appear. If we just scroll down a bit then we can see our artifacts also, let‚Äôs check the ROC curve for this experiment It is also possible to see the comparison of different experiments side by side. This feature is immensely helpful to trace the results and impact of the features on the performance of the model. Go to the home page and click on compare by selecting the different model run. So, we can see MLflow provides a lot of cool features and with the few lines of the code, we can achieve all complex things which used to be difficult and cumbersome before the inception of MLflow. Some highlights of MLflow API MLflow API is well designed and regularly new features are being added. It is worth checking the API to be in sync with new features and changes. However, I would like to highlight a few interesting features of MLflow MLflow API is not only about Python but it also supports Java, and R programming language by the time of writing this blog. Soon Scala will also be the part of the API. REST API is also available which facilitates create, list, get operations on experiments, runs, log parameters, metrics, and artifacts. Auto-logging is worth using for deep learning models. As we know, Deep Learning models capture several parameter/hyper-parameters during the training of the model and each value is not always possible to log by using mlflow.log_metric. Doing manual captures can lead to miss some important metrics. So, to make it simple, MLflow comes with auto-logging. It is very simple to use and merely enabling it ensures to capture and log every possible metric. Autolog feature is available for Keras, Tensorflow, Gluon, LighGBM, XGBoost, and Spark . Visit documentation and my github to see the usage of it. Model tracking on the local file system is good for the experiment's purpose. To keep track of a model in production, it is good practice to save all metadata, data, artifacts in cloud storage, or SQL databases and use a separate dedicated server for better tracking and maintenance. There are many more features available that can be studied in the documentation and used as per the different use-cases. Conclusion Overall, we have seen the power of MLflow and also learned that no matter which framework, the programming language is used to develop a model, MLflow is capable to provide a robust mechanism with just a few lines of the code to track the model development, tuning, packaging, and reproducibility. This one is a must-have tool in the machine learning arsenal. References https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf This 3 series of MLflow workshop is highly recommended https://www.youtube.com/playlist?list=PLTPXxbhUt-YWjDg318nmSxRqTgZFWQ2ZC www.mlflow.org https://databricks.com/product/managed-mlflow Managing Machine Learning Life cycle with MLflow was originally published in Analytics Vidhya on Medium, where people are continuing the conversation by highlighting and responding to this story.\n\nPhoto by Angelo Moleele on Unsplash In the machine learning decision process, it is often said that simpler models are easy to explain and understand. But, we know most of the time simpler models don‚Äôt perform well, and to achieve better performance and accuracy, it becomes a necessity to rely on complex models which are again treated as a black box when it comes to explaining to the business users or decision-makers. So, in general, there is a trade-off between simple and complex models. But recent breakthroughs in the interpretability of the machine learning model( LIME and SHAP ) have provided a useful guide that effectively can be used to explain the behavior of the model. The fundamental principles behind these two concepts are simple and explainable. LIME takes the importance of local features and SHAP treats the collective or individual feature contribution towards the target variable. So, if we can explain the model lucidly then interpretability could be a great technique To explain the model to business users. To help Data scientists to understand the importance of features from a business perspective. To develop confidence and trust in the developed model. To help in debugging and validation of the model. In this blog, I will cover popular techniques and their implementation to interpret the models. SHAP (Shapley Additive Explanations) SHAP was proposed by Scott Lundberg and Su-In Lee in 2017 and the original paper can be found here The SHAP algorithm is a model agnostic and proposed techniques drastically reduces the complexity of additive feature-attribution methods from O(TLD^M) to O(TLD¬≤) where T and M are the number of trees and features, respectively, and D and L are the maximum depth and number of leaves across the trees. The open-source SHAP library worked well with almost every popular ensemble model like XGBoost, LightGBM, CatBoost, and sklearn tree models. SHAP algorithm is inspired by the Cooperative Game theory and uses Shapley Values to explain features by the contribution of the weight(coefficients). Usage of SHAP in Machine learning For the simple model, it is easy to explain the importance and impact of feature(s) intuitively e.g. how associated weights are calculated for prediction, why a particular decision tree was formed for a class, etc. However, for the complex models this is not very straightforward, and here comes the idea of Shapely Value to explain how an individual or a group of features affects the predicted target. In general, the Shapley value is calculated as an average of the marginal contributions made by each player in the coalition of the players. It takes the consideration of fair distribution among the players for the ‚Äúpayout‚Äù. To understand the math and details, please have a look at the original paper The basic understanding of Cooperative Game Theory terminologies and its axioms are essentials to understand the calculation of Shapely Values. Cooperative Game Theory: A cooperative game (or coalitional game) is a game with competition between groups of players due to the possibility of external enforcement of cooperative behavior. Players: Agents(players) participating in the game. Coalitions: Temporary alliance between the player to contribute to the game. Fair distribution: How fair contribution is distributed among players? Axioms i) Efficiency: The sum of the Shapley values of all agents equals the value of the grand coalition so that all the gain is distributed among the agents. where N = total number of agents, œÜ(v)=shapley value, v(N)= grand coalitions for all players. ii) Symmetry: All players have a fair chance to join the game. This property is also called the equal treatment of equals. If i and j are two actors who are equivalent in the sense that For every subset S of N which contains neither i or j then iii) Linearity: If two coalition games described by gain functions v and w are combined, then the distributed gains should correspond to the gains derived from v and the gains derived from w. for every i in N. iv) Null or Dummy Player: The Shapley value of a null player i in a game v is zero. A player i is null in v if for all coalitions S that do not contain i.e. if the player contributes nothing to coalitions then the contribution will be zero. In real life, a game can be translated into any action e.g. sharing the taxi/foods, playing games, working for collaborative action, or predicting outcomes of any classification or regression model by using the available feature(s). To get a fair idea about the calculation of Shapley Value, let‚Äôs think of a simple example. Three friends F1, F2, and F3 are going to share the taxi to reach their home. If they go individually then each needs to pay $10, $15, and $30 respectively. If they travel in a group then the maximum payment will be $30. But the question is ‚Äî Apart from the general average pay contribution [(10+15+30)/3], how can we calculate fair individual pay from a coalition? And here comes, the famous Shapley Value for rescue. As per the given details we know if each friend will travel to their home individually then they will pay F1-> $ 10 F2 -> $ 15 F3 -> $ 30 However, if they are going to travel together then the below combination will be formed Now, let‚Äôs create a possible permutation if they travel in a group with given coalitions. How did we calculate our permutation sets? {F1, F2, F3} => If F1 & F2 will form a coalition then they will pay together $15. Since F1 get down first then he will pay $10 and the remaining amount $5 will be given by F2 and $15 will be payable by F3 (Since $15 is already paid) {F1, F3, F2} => This time, coalition will be formed by F1 and F3. So F1 will pay $10 and the remaining amount will be paid by F3 which is $30‚Äì$10=$20. And when a coalition will be formed between F3 and F2, F2 will not have to pay anything. In the same way, other permutations set and individual contributions are calculated. The calculated averages are Shapley value which gives individuals pay for all friends if they travel together. And if we sum up the average value of all friends and coalition pay of the permutation set then it always ends up with 30 which shows the payments are fairly distributed. The same concept is also relevant when we visualize the importance of features and their weightage during the prediction of the target variable. We can think of these calculated Shapley values as their contributions towards the prediction of a certain class or value. In machine learning, we can relate above understanding like this. Game -> Model Players -> Features Subset of Players -> Subset of Features Shapley value of Features -> contribution of the feature. LIME (Local Interpretable Model-Agnostic Explanations) LIME is also a popular algorithm proposed by Marco Tulio, Sameer Singh, and Carlos Guestrin. with the title ‚Äú Why should I trust you ‚Äù This algorithm uses the local explanation to describe the importance of features. The explanation can be generated by approximating the underlying model by a simple linear model with few non-zero coefficients. Mathematical representation of LIME is So the explanation of the model measures how close the explanation is to the prediction of the original model(f) by keeping model complexity low. where x = instance, L = Locality aware Loss function, f= model to explain(original model), g = linear model, G= Explanation model family, Pi - proximity measure of the instance x to define locality and Omega(g) = model complexity. To get an overview of LIME, let us understand the meaning of each word. Local: Locally reliable explanation. This is the key intuition behind the LIME algorithm. Interpretable: Information that can be processed and understood. If we are processing text then it shows presence/absence of words, for images it is presence/absence of superpixels , and for tabular data it weighted combination of features (coeffecients). Model-Agnostic: Works with any type of predictive model i.e. it shouldn‚Äôt make any assumptions about model while providing explanations. Explanations: Relationship and understanding of the model‚Äôs input and output. Steps to perform LIME i) Choose an observation that we want to explain. ii) Around the observation, each data point is perturbed and the neighborhood is generated. Irrespective of the complexities of the model, at the local level, it is possible to divide the region linearly (local linear regression). In perturbation, out of all features set to 1 (for a given data point), randomly sample a subset of features, and set their values to 0. See the below diagram from its original paper In the above picture, we can see those decision boundaries are quite complex and it is tough for any simple model to separate it globally. However, if we narrow down and reach a single data point then locally it is easy to separate the decision boundary with the linear model. iii) Calculate the distance between new points and the observation (the measure of similarity). iv) Find the subset of m features that has the strongest relationship with our target class. v) Fit a linear model or any interpretable model on the dataset in n- dimensions weighted by similarity. vi) Weights of the linear models are used as explanations of decisions and we know simple models are explainable. Drawbacks i) It depends on the random sampling of new points so it could be unstable. ii) Fit for linear models can be inaccurate. However, LIME provides R¬≤ value and the same can be checked to see whether locally linear regression is a good fit or not. iii) As we need to generate a sample dataset for each observation to explain, this makes it relatively slow for a single observation, in particular with images. Permutation Importance or Mean Decrease Accuracy (MDA) This is also one of the techniques used for model explainability. It measures how does score decrease if a feature is not available. The algorithm of this method is very simple ‚Äî Remove features and retrain the model. Check the R¬≤, F1 value, and model‚Äôs accuracy. So, any changes in these values can explain the importance of features. Drawbacks Computationally expensive as it required re-training of the model. However, it can be avoided by generating noise data for removed columns in the test dataset by the same distribution as the original feature value. Doing this will help to avoid re-training the model but it needs a careful population of data. Partial dependence plots (PDP) Partial dependence plots are a useful technique to represents the marginal effects of the independent variables (features) on the dependent variable(target) with the average effect of all other independent variables . Once the models are trained, then by plotting PDP, we can visualize the graph and relate the interactions of features with the target. Drawbacks PDP assumes that the features for which the plot is created are not correlated with other features. Limited visualization techniques (one PDP plot cannot show more than 3 features. Plot gets messy and hard to read) Available Python packages Below are the few popular python packages which can be used to explain the importance of the features. shap, LIME, Skater, azureml-interpret, azureml-interpret-contrib, ELI5 (Explain Like I‚Äôm 5) ‚Üí provides the implementation of MDA . sklearn.ensemble module provides partial_dependence function. Let‚Äôs see the interpretability in the code The idea of the below code snippet is not making a good model but showing how existing state-of-art techniques can be used to understand the prediction in a better way. the complete notebook can be found here Required Libraries sklearn, numpy, matplotlib, shap, azureml, eli5 from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report import numpy as np import matplotlib.pyplot as plt import shap from interpret.ext.blackbox import TabularExplainer from interpret_community.widget import ExplanationDashboard shap.initjs() I‚Äôm loading breast cancer dataset to show how model interpretability can be explained on a complex model like Random Forest. cancer_dataset = shap.datasets.sklearn.datasets.load_breast_cancer() feature_names = cancer_dataset.feature_names class_names = [\"Malignant(0)\", \"Benign(1)\"] X_train, X_test, Y_train, Y_test = train_test_split(cancer_dataset.data, cancer_dataset.target, test_size=0.25, random_state=23) rfc = RandomForestClassifier() rfc.fit(X_train, Y_train) Before using the interpretable library, let‚Äôs see which features are important for forest tree def plot_imp_features(model, top_n): try: tree_feature_importances = rfc.feature_importances_ except Exception as e: raise TypeError(f\"Model must be a type of DecisionTree to use feature_importances_ Excpetion is {e}\") sorted_idx = tree_feature_importances.argsort() select_n = sorted_idx[:top_n] y_ticks = np.arange(0, len(select_n)) fig, ax = plt.subplots() ax.barh(y_ticks, tree_feature_importances[select_n]) ax.set_yticklabels(feature_names[select_n]) ax.set_yticks(y_ticks) ax.set_title(\"Random Forest Feature Importances (MDI)\") fig.tight_layout() plt.show() plot_imp_features(rfc, 15) print(classification_report(Y_test, rfc.predict(X_test), target_names=class_names)) Now, time to use shap lib to see how can we explain the impact of the features on the target class explainer = shap.TreeExplainer(rfc, X_train, link=\"logit\") shap_values = explainer.shap_values(X_test, check_additivity=False) In the below force_plot two color codes are used to plot a relationship between the shap value and features. To grasp the idea of the plot, first we need to understand the color codes which are used in the graph. With this understanding, let us see how the below features are explaining the outcomes of the model. #first 10 classes for X_test dataset Y_test[:10] # (array([1, 1, 0, 1, 0, 0, 0, 1, 1, 0]),) expected_index = 1 # indexes are 1,0 test_data_index = 1 # test_data_index can range from 0 to #len(test_data_index) shap.force_plot(explainer.expected_value[expected_index], shap_values[expected_index][test_data_index,:], X_test[test_data_index,:], feature_names=feature_names) Force plot shows the cumulative impact of various features and their values on the model output. In the above diagram, the predicted value is 0.60; and the mean concave points is pushing the predictions from 0.5167 to 0.60 If we follow the blue color code then we can see that other features are minimizing the impact and pushing all prediction towards 0 i.e. Malignant and worst perimeter is contributing to the bigger margin. Now, let‚Äôs analyze the second data point and here we can see that features e.g. worst perimeter, mean concave points, mean area, worst radius, etc are pushing the prediction towards 0 i.e. Malignant expected_index, test_data_index = 1, 2 #test_data_index can range #from 0 to len(test_data_index) shap.force_plot(explainer.expected_value[expected_index], shap_values[expected_index][test_data_index, :], X_test[test_data_index, :], feature_names=feature_names) To get the explanations of entire dataset, we can pick many explanations shown above, rotate them 90 degrees, and then stack them horizontally. i=0 # class label. Can be change to 1 to see the behavior shap.force_plot(explainer.expected_value[i], shap_values[i], features=X_test, feature_names=feature_names) shap also provides another type of plot named summary plot. In this plot, the middle line represents shap value=0, and deviation (+ve to -ve value) from midline shows how does a particular feature impact the prediction of the target value. i=1 shap.summary_plot(shap_values[i], features=X_test, feature_names=feature_names, class_names=class_names) summary plot draw the diagram by keeping feature on X-axis and it‚Äôs SHAP value in Y-axis Understanding a single feature: The function automatically includes another variable that most interacts with the chosen one. The below plot depicts that there is a negative trend between mean texture and the target variable , and the worst perimeter interacts with mean texture frequently. This piece of code can be used to visualize the SHAP values for each class and features for feature_name in range(len(feature_names)): for target_label in range(len(cancer_dataset.target_names)): shap.dependence_plot(feature_name, shap_values[target_label], features=X_test, feature_names=feature_names) SHAP lib also contains other explainers like PermutationExplainer, GradientExplainer, KernelExplainer, LinearExplainer, DeepExplainer, etc. In API we can see in detail about these explainers dir(shap) ['BruteForceExplainer', 'DeepExplainer', 'GradientExplainer', 'KernelExplainer', 'LinearExplainer', 'PartitionExplainer', 'PermutationExplainer', 'SamplingExplainer', 'Tree', 'TreeExplainer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_cext', 'approximate_interactions', 'bar_plot', 'common', 'datasets', 'decision_plot', 'dependence_plot', 'embedding_plot', 'explainers', 'force_plot', 'have_matplotlib', 'hclust_ordering', 'image_plot', 'initjs', 'kmeans', 'matplotlib', 'monitoring_plot', 'multioutput_decision_plot', 'other', 'partial_dependence_plot', 'plots', 'sample', 'save_html', 'summary_plot', 'unsupported', 'warnings', 'waterfall_plot'] LIME (Local Interpretable Model-Agnostic Explanations) LIME is also very simple to use. First, we need to create a particular explainer for our dataset and then we can call the explain_instance method of explainer instance to explain the original model. from lime.lime_tabular import LimeTabularExplainer np.random.seed(11) tab_explainer=LimeTabularExplainer(X_train,feature_names=feature_nam es,class_names=class_names, discretize_continuous=True,random_state=11) def explain_model(model, test_data, index=0, num_features=1, top_labels=1): if test_data.size ==0: raise ValueError(\"Empty dataset is passed\") expl = tab_explainer.explain_instance(test_data[index], model.predict_proba, num_features=num_features, top_labels=top_labels) expl.show_in_notebook(show_table=True,show_all=False) explain_model(rfc, X_test, index=0, num_features=10) # Y_test[1] = 1(Benign) In the above diagram, we can see that model predicted the benign class and the highlighted features are making a bigger impact. On the right side, we can see the worst area got the value of 517.88 which is less than its actual value i.e 564.20. We can check the above value from the dataset. cancer_dataset.feature_names[-7] # worst area X_test[0][-7] Out[196]: 564.2 cancer_dataset.feature_names[-8] # worst perimeter X_test[0][-8] Out[197]: 86.6 Partial dependence plot scikit-learn provides an inbuilt implementation of PDP. partial_dependence function accept BaseGradientBoosting fitted model as a parameter to provide the PDP. So, for this reason, I‚Äôm training a new classifier. from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import partial_dependence gbc = GradientBoostingClassifier() gbc.fit(X_train, Y_train) # [20] is the index of columns of train data for PDP partial_dependence.plot_partial_dependence(gbc, X_train, [20], feature_names=feature_names) The plot is showing how the worst radius is interacting with the target (malignant and benign). As soon as the size of the worst radius is crossing the mark of 16, the prediction sharply falls from benign to malignant class. AzureML Azure has also published an open-source library under its azuleml SDK. This library can be used for all type of machine learning models which follows the convention of scikit-learn methodologies i.e. fit() and transform() If we are working with any custom algorithm then we can simply wrap our main model inside function fit() and pass it to azureml to explain the features. Below interpretability chart is useful to understand different explainers https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability Now, if we pass the same random forest classifier as a parameter to TabularExplainer then we can get the explanation of the model. tab_explainer = TabularExplainer(rfc, X_train, features=feature_names, classes=[0, 1]) global_explanation = tab_explainer.explain_global(X_test) # How globally all features set interpretate the outcome global_feature_rank_values = dict(zip(global_explanation.get_ranked_global_names(), global_explanation.get_ranked_global_values())) global_feature_rank_values Out[23]: {'worst perimeter': 0.07595532987463141, 'worst concave points': 0.06780037332226518, 'worst radius': 0.06178580090500718, 'worst area': 0.061688813644569004, 'worst concavity': 0.051432037342211356, 'mean area': 0.02544095929461229, 'mean concavity': 0.022822935606762865, 'mean perimeter': 0.02230714237151857, 'mean concave points': 0.014994114385124891, 'mean texture': 0.01426150261834639, 'mean radius': 0.012523284263610842, 'worst texture': 0.012036364340557092, 'worst compactness': 0.011888211618859144, 'worst smoothness': 0.007687756633585173, 'mean compactness': 0.007227320108198555, 'perimeter error': 0.006870732294207836, 'area error': 0.006006945019138089, 'worst symmetry': 0.0059455597100683925, 'radius error': 0.00498892192057288, 'compactness error': 0.00440812326792559, 'mean fractal dimension': 0.004065060877333495, 'concavity error': 0.003417519253002984, 'texture error': 0.003318578728239531, 'worst fractal dimension': 0.0029870047227208156, 'symmetry error': 0.00259608337064409, 'mean smoothness': 0.0018153337349097675, 'concave points error': 0.0015773339816971782, 'fractal dimension error': 0.001495528819412792, 'smoothness error': 0.0007389462921350583, 'mean symmetry': 0.0005036257960195172} Azureml also provides ExplanationDashboard, which is a powerful Dashboard Service. It provides 4 basic plots for the model explanation. Data Exploration: In this tab, we can choose the X and Y axis value to see the respective target distribution. On the right top corner, we can see the target class Global Importance: Global importance variables and we can select top K features Explanation Exploration: This tab, explain why a particular feature important for prediction. Summary Importance: Summary of all features and their relationship with the target variable. ExplanationDashboard(global_explanation, rfc , datasetX=X_test) Conclusion To conclude, we can say that adopting the interpretability as a feedback loop can enhance the trust over the model. The simple and logical interpretation immensely adds business values to the stakeholders and provides confidence in the model to perform sensibly as expected. So, along with the various measures for the model (e.g. accuracy, cross-validation) interpretability should also get its own place in the model development life cycle. References https://christophm.github.io/interpretable-ml-book/ https://arxiv.org/abs/1705.07874 https://arxiv.org/abs/1602.04938 ‚Äî Why Should I Trust You. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability?WT.mc_id=azuremedium-blog-lazzeri https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability https://books.google.nl/books?id=wlexDwAAQBAJ&pg=SA6-PA21&lpg=SA6-PA21&dq=azureml+Tabular+explainer&source=bl&ots=7_Stxdaw0l&sig=ACfU3U12IPmJI0ImltJIitWY3VO1vdPGeg&hl=en&sa=X&ved=2ahUKEwiDu56gtM3pAhVLwAIHHerQCa0Q6AEwBXoECAoQAQ#v=onepage&q=azureml%20Tabular%20explainerTreeScoringExplainer&f=false https://www.youtube.com/watch?v=C80SQe16Rao&t=2334s https://scikit-learn.org/stable/modules/partial_dependence.html https://eli5.readthedocs.io/en/latest/# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability Interpretability of Machine Learning models was originally published in Analytics Vidhya on Medium, where people are continuing the conversation by highlighting and responding to this story."
}